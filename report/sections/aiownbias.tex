\section{ai own biases and influenceability}
\label{sec:ai_biases}

Since artificial agents are trained on mostly human-generated data, they learn human biases and tendencies themselves, developing both historical and political preferences in the textual content they generate \cite{roselli_managing_2019,rozado_political_2023,rozado_political_2024}. They tend to exhibit social sycophancy, by agreeing with and flattering the user at the cost of correctness \cite{cheng_elephant_2025}, and they are easily influenced by small changes in prompt wording \cite{sclar_quantifying_2024, salinas_butterfly_2024}. This can raise ethical concerns as biased AIs lead to discrimination or exclusion of marginalized groups. \cite{rosario_generative_1}.

%% measure ai bias on us politics: 2503.1064

Rozado, in the context of US politics, measured the political bias of many popular large language models \cite{rozado_measuring_2025}. First, they calculated the similarity between AI-generated text and public speeches from Congress representatives (both Democrat and Republican). Then, they used an LLM to annotate as left- or right-leaning AI-generated policy recommendations. Consequently, they did a sentiment analysis on AI-generated comments about american public figures, such as legislators or journalists (figure \ref{fig:rozado_colors}), and lastly they administered three different political orientation tests to the various LLMs. The results show substantial evidence that the models are biased and left-leaning. Table \ref{tab:rozado} shows the three most and three least biased LLMs among those tested.

\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{images/rozando_manycolors.png}
  \caption{Average sentiment (negative: -1, neutral: 0, positive: 1) towards ideologically aligned public figures in conversational LLMsâ€™ generated texts. Statistically significant two-sample t-tests at the 0.01 threshold are indicated with an asterisk. Rozado \cite{rozado_measuring_2025}}
  \Description{Many bar graphs.}
  \label{fig:rozado_colors}
\end{figure}

\begin{table}[h]
    \centering
    \caption{Ranking of political bias in conversational LLMs sorted in ascending
order from least politically biased to most. Rozado \cite{rozado_measuring_2025}}
    \label{tab:rozado}
    \begin{tabular}{lc}
        \toprule
        \textbf{Rank} & \textbf{Model}\\
        \midrule
            1   &   Google Gemma 1.1 2b IT  \\
            2   &   xAI Grok Beta    \\
            3   &   Mistral AI Mistral 7B Instruct v0.2     \\
            ... &   ...     \\
            18  &   Nous Hermes 2 Mixtral 8x7B DPO     \\
            19  &   Google Gemini 1.5 Pro      \\
            20  &  Google Gemini 1.5 Flash      \\
        \bottomrule
    \end{tabular}
\end{table}

%% influence ai opinions/beliefs: 2510.19107

As previously stated, AIs can be easily influenceable. Mehdizadeh and Hilbert \cite{mehdizadeh_when_2025} test such a theory by subjecting LLMs to peer-pressure. The models are immersed into fictitious social networks (figure \ref{fig:mehdi_sn}), and each agent periodically receives a natural language prompt summarizing the opinions of its immediate neighbors. It then decides whether to update its opinions or not. The flowchart of the simulation can be seen at figure \ref{fig:mehdi_flowchart}. 

Overall, the study highlights two main results: agents' malleability depends on the model (\textit{Gemini 1.5 Flash} requires over 70\% peer disagreement to flip, whereas \textit{ChatGPT-4o-mini} shifts with a dissenting minority), and different cognitive orientations respond differently to outside stimuli. For \textit{values} and \textit{opinions}, agents show a strong resistance to abandoning a "Yes" stance, making them robust once affirmed , whereas for \textit{attitudes} and \textit{intentions} the greatest challenge is overcoming a negative "No". On the other hand, \textit{beliefs} display a near-perfect symmetry.

\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{images/mezide_san.png}
  \caption{The then 100-nodes networks used in the study. Mehdizadeh and Hilbert \cite{mehdizadeh_when_2025}.}
  \Description{Many network graphs.}
  \label{fig:mehdi_sn}
\end{figure}

\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{images/mehdi_flcrt.png}
  \caption{Flowchart of the simulation procedure in the LLM-driven network model. The process continues asynchronously across the network until a specified number of steps or convergence criterion is met. The setup measures how local peer influence, mediated through LLM interpretation, shapes the emergent collective opinion. Mehdizadeh and Hilbert \cite{mehdizadeh_when_2025}}
  \Description{Many network graphs.}
  \label{fig:mehdi_flowchart}
\end{figure}

In conclusion, the literature offers empirical proof that artificial agents inherit biases from their training data and are influenced by external stimuli. This, combined with their ability to manipulate humans' opinions (section \ref{sec:ai_humancentric}), raises ethical concerns that must be addressed.  